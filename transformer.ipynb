{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1809d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext.datasets as datasets\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d7bc67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "91f64369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, context_len, embedding_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.kqv_proj = nn.Linear(embedding_dim, 3*embedding_dim)\n",
    "        self.out_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(self.dropout)\n",
    "        self.resid_dropout = nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        self.attn_mask = None\n",
    "    \n",
    "    def get_attn_mask(self):\n",
    "        if self.attn_mask is None:\n",
    "            # We register a buffer to not store this mask as a model\n",
    "            # parameter and thus not update it while training!\n",
    "            self.attn_mask = self.register_buffer(\n",
    "                \"mask\",\n",
    "                torch.ones(context_len, context_len)\n",
    "                    .view(1, 1, context_len, context_len)\n",
    "            )\n",
    "        \n",
    "        return self.attn_mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # batch size, sequence length, embedding dimensionality\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # we get the k, q, v projection of each embedding, each\n",
    "        # matrix will have dimension (B, T, C)\n",
    "        k, q, v = self.kqv_proj(x).split(self.embedding_dim, dim=2)\n",
    "        \n",
    "        # next we split the projected embeddings across the number\n",
    "        # of heads we have, allowing each head to gain a different\n",
    "        # interpretation.\n",
    "        # (B, num_heads, T, head_size)\n",
    "        head_size = C // self.num_heads\n",
    "        k = k.view(B, T, self.num_heads, head_size).transpose(1, 2)\n",
    "        q = q.view(B, T, self.num_heads, head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, head_size).transpose(1, 2)\n",
    "        \n",
    "        mask = self.get_attn_mask()\n",
    "        \n",
    "        if self.flash:\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                attn_mask = mask,\n",
    "                dropout_p = self.dropout if self.training else 0.0\n",
    "            )\n",
    "        else:\n",
    "            # (B, num_heads, T, head_size) x (B, num_heads, head_size, T) -> (B, num_heads, T, T)\n",
    "            attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            \n",
    "            # attend to only past tokens by masking out future tokens\n",
    "            attn = att.masked_fill(mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "            \n",
    "            attn = F.softmax(attn, dim = -1)\n",
    "            attn = self.attn_dropout(attn)\n",
    "            \n",
    "            # (B, num_heads, T, T) x (B, num_heads, T, head_size) -> (B, num_heads, T, head_size)\n",
    "            out = attn @ v\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        out = self.resid_dropout(self.out_proj(out))\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "141ffb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalMultiHeadAttention(MultiHeadAttention):\n",
    "    \n",
    "    def __init__(self, context_len, embedding_dim, num_heads, dropout=0.1):\n",
    "        super().__init__(context_len, embedding_dim, num_heads, dropout)\n",
    "        \n",
    "        # Causal attention allows tokens to attend to only\n",
    "        # previous tokens, token t_i can also look at\n",
    "        # tokens t_0:i-1\n",
    "        self.causal_attn_mask = self.register_buffer(\n",
    "            \"causal_mask\",\n",
    "            torch.ones(context_len, context_len)\n",
    "                .tril()\n",
    "                .view(1, 1, context_len, context_len)\n",
    "        )\n",
    "    \n",
    "    def get_attn_mask(self):\n",
    "        return self.causal_attn_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "194dbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, ff_dim, dropout=0.10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.inter_rep = nn.Linear(embedding_dim, ff_dim)\n",
    "        self.out_proj = nn.Linear(ff_dim, embedding_dim)\n",
    "        \n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.inter_rep(x)\n",
    "        x = self.out_proj(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "38f6a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, attn_mechanism, ffn, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln_1 = nn.LayerNorm(embedding_dim)\n",
    "        self.attn = attn_mechanism\n",
    "        \n",
    "        self.ln_2 = nn.LayerNorm(embedding_dim)\n",
    "        self.ffn = ffn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.ln_1(x)\n",
    "        x = x + self.attn(x)\n",
    "        \n",
    "        x = self.ln_2(x)\n",
    "        x = x + self.ffn(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c2bc408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    CONTEXT_LENGTH = 1024\n",
    "    VOCAB_SIZE = 50304\n",
    "    EMBEDDING_DIM = 768\n",
    "    INTER_DIM = 2048\n",
    "    NUM_HEADS = 12\n",
    "    NUM_LAYERS = 12\n",
    "    DROPOUT = 0.0\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                self.EMBEDDING_DIM,\n",
    "                CausalMultiHeadAttention(\n",
    "                    self.CONTEXT_LENGTH,\n",
    "                    self.EMBEDDING_DIM,\n",
    "                    self.NUM_HEADS,\n",
    "                    self.DROPOUT\n",
    "                ),\n",
    "                FFN(self.EMBEDDING_DIM, self.INTER_DIM, self.DROPOUT),\n",
    "                self.DROPOUT\n",
    "            ) for _ in range(self.NUM_LAYERS)\n",
    "        ])\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(self.VOCAB_SIZE, self.EMBEDDING_DIM),\n",
    "            wpe = nn.Embedding(self.CONTEXT_LENGTH, self.EMBEDDING_DIM),\n",
    "            dropout = nn.Dropout(self.DROPOUT),\n",
    "            blocks = blocks,\n",
    "            ln = nn.LayerNorm(self.EMBEDDING_DIM)\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(self.EMBEDDING_DIM, self.VOCAB_SIZE, bias = False)\n",
    "        \n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        for param_name, params in self.named_parameters():\n",
    "            if param_name.endswith('out_proj.weight'):\n",
    "                torch.nn.init.normal_(\n",
    "                    params,\n",
    "                    mean = 0.0,\n",
    "                    std = 0.02/math.sqrt(2 * self.NUM_LAYERS)\n",
    "                )\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, input_ids, targets = None):\n",
    "        device = input_ids.device\n",
    "        \n",
    "        B, T = input_ids.size()\n",
    "        assert T <= self.CONTEXT_LENGTH\n",
    "        \n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "        \n",
    "        token_embeddings = self.transformer.wte(input_ids)\n",
    "        pos_embeddings = self.transformer.wpe(pos)\n",
    "        \n",
    "        # (B, T, embedding_dim)\n",
    "        x = token_embeddings + pos_embeddings\n",
    "        x = self.transformer.dropout(x)\n",
    "        \n",
    "        for block in self.transformer.blocks:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            # (B, T, vocab_size)\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index = -1\n",
    "            )\n",
    "        else:\n",
    "            # (B, 1, vocab_size)\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def configure_optimizer(self, weight_decay, learning_rate, betas, device_type):\n",
    "        params = {param_name : param for param_name, param in self.named_parameters()}\n",
    "        \n",
    "        decay_params = []\n",
    "        nodecay_params = []\n",
    "        for param in params.values():\n",
    "            if param.dim() >= 2:\n",
    "                decay_params.append(param)\n",
    "            else:\n",
    "                nodecay_params.append(param)\n",
    "                \n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        \n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            context_window = input_ids\n",
    "            if context_window.size(1) > self.CONTEXT_LENGTH:\n",
    "                context_window = input_ids[:, -self.CONTEXT_LENGTH:]\n",
    "            \n",
    "            logits, _ = self(context_window)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            input_ids = torch.cat((input_ids, next_id), dim=1)\n",
    "        \n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e88c572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT()\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2cf4985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31373, 616, 1438, 318, 407]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello my name is not\"\n",
    "input_ids = enc.encode(text)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d327024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(input_ids, dtype=torch.long, device=device).view(1,-1)\n",
    "out = model.generate(x, 2, 0.6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "018972e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello my name is not not not'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gibberish since the model is not trained\n",
    "enc.decode(out.flatten().tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
